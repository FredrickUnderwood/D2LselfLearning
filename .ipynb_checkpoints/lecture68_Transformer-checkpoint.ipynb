{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "https://nn.labml.ai/transformers/index.html\n",
    "1. multi-head attention https://nn.labml.ai/transformers/mha.html\n",
    "2. encoder and decoder https://nn.labml.ai/transformers/models.htmlch\n",
    "3. feed forward network https://nn.labml.ai/transformers/feed_forward.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Is All You Need\n",
    "### 自注意力机制 Scaled Dot-Product Attention\n",
    "查询（Query）、键（Key）和值（Value）\n",
    "$$ \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V \\] $$\n",
    "a (query) Q: n * d_k (d_k: dimension of key)\n",
    "a set of (key-value) set: value前的参数取决于key和query的相似度 K: m * d_k; V: m * d_v (d_v: dimension of value)\n",
    "an (output)： a weighted sum of the values; output: n * d_v\n",
    "Q与K做内积，内积越大，相似度越高；内积为0，则是正交向量，相似度低\n",
    "#### 为什么除以sqrt(d_k)？\n",
    "如果d_k过大，可能导致对应的QK^T过大，导致softmax后的数据过于集中，因此会过早收敛"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## multi-head attention\n",
    "### 多头注意力机制的引入\n",
    "时序神经网络不能进行并行计算，因此计算的性能很差\n",
    "为了解决这些问题，有ConvS2S这种用卷积神经网络代替的方法，但是卷积神经网络的input和output位置是随机的，难以学习一张图上距离较远的两个位置之间的关系\n",
    "因此引入Transformer的多头注意力机制\n",
    "### 多头注意力机制\n",
    "1.先定义一个head的个数h\n",
    "2.V、K、Q先各通过一个Linear网络投影到低维（这个Linear网络有参数w可以学习）\n",
    "d_k = d_v = d_model / h\n",
    "3.h组低维的V、K、Q进入Scaled Dot-Product Attention\n",
    "4.将结果concat\n",
    "5.进入一个Linear网络后输出（这个Linear网络有参数w可以学习）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## encoder and decoder\n",
    "### 编码器和解码器的结构\n",
    "编码器：(x1, x2, ... , xn) -> (z1, z2, ... , zn) 连续生成\n",
    "解码器：(z1, z2, ... , zn) -> (y1, y2, ... , ym) 一个一个生成，输出vector的长度不同\n",
    "解码器的auto_regressive：(z1, z2, ... , zn)先生成y1，之后(z1, z2, ... , zn)和y1生成y2，(z1, z2, ... , zn)再和(y1, y2)生成y3，以此类推。因此过去时刻的输出会成为你当前时刻的输入\n",
    "### encoder\n",
    "sub-layer1: multi-head attention\n",
    "sub-layer2: feed forward\n",
    "每个sub-layer后增加一个residual层和一个layer-normalization层\n",
    "LayerNorm(x + SubLayer(x))\n",
    "每一个层输出的维度d_model = 512（feature的维度）\n",
    "### why layer-normalization not batch-normalization?\n",
    "batch-normalization对每个batch的每个feature做均值和方差\n",
    "layer-normalization对每个样本做均值和方差\n",
    "对于数据整体有三个维度，batch_size，seq，feature，因为每个seq的长度可能不同\n",
    "如果对每个feature求均值和方差，每个feature对应不同的seq的长度不同\n",
    "### decoder\n",
    "sub-layer1: masked multi-head attention \n",
    "masked multi-head attention 防止output在i后面的序列影响i的预测\n",
    "sub-layer2: multi-head attention\n",
    "sub-layer3: feed forward\n",
    "每个sub-layer后增加一个residual层和一个layer-normalization层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## feed forward network\n",
    "### 前馈神经网络\n",
    "position-wise fully connected feed forward network 其实就是一个简单的MLP "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
