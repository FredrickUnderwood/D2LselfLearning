{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Attention Is All You Need\n",
    "[paper_link](https://nn.labml.ai/transformers/index.html)\n",
    "1. multi-head attention https://nn.labml.ai/transformers/mha.html\n",
    "2. encoder and decoder https://nn.labml.ai/transformers/models.htmlch\n",
    "3. feed forward network https://nn.labml.ai/transformers/feed_forward.html\n",
    "### 自注意力机制 Scaled Dot-Product Attention\n",
    "查询（Query）、键（Key）和值（Value）\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V\n",
    "$$\n",
    "a (query) Q: n * d_k (d_k: dimension of key)\n",
    "a set of (key-value) set: value前的参数取决于key和query的相似度 K: m * d_k; V: m * d_v (d_v: dimension of value)\n",
    "an (output)： a weighted sum of the values; output: n * d_v\n",
    "Q与K做内积，内积越大，相似度越高；内积为0，则是正交向量，相似度低\n",
    "#### 为什么除以sqrt(d_k)？\n",
    "如果d_k过大，可能导致对应的QK^T过大，导致softmax后的数据过于集中，因此会过早收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### multi-head attention\n",
    "#### 多头注意力机制的引入\n",
    "时序神经网络不能进行并行计算，因此计算的性能很差\n",
    "为了解决这些问题，有ConvS2S这种用卷积神经网络代替的方法，但是卷积神经网络的input和output位置是随机的，难以学习一张图上距离较远的两个位置之间的关系\n",
    "因此引入Transformer的多头注意力机制\n",
    "#### 多头注意力机制\n",
    "1.先定义一个head的个数h\n",
    "2.V、K、Q先各通过一个Linear网络投影到低维（这个Linear网络有参数w可以学习）\n",
    "d_k = d_v = d_model / h\n",
    "3.h组低维的V、K、Q进入Scaled Dot-Product Attention\n",
    "4.将结果concat\n",
    "5.进入一个Linear网络后输出（这个Linear网络有参数w可以学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### encoder and decoder\n",
    "#### 编码器和解码器的结构\n",
    "编码器：(x1, x2, ... , xn) -> (z1, z2, ... , zn) 连续生成\n",
    "解码器：(z1, z2, ... , zn) -> (y1, y2, ... , ym) 一个一个生成，输出vector的长度不同\n",
    "解码器的auto_regressive：(z1, z2, ... , zn)先生成y1，之后(z1, z2, ... , zn)和y1生成y2，(z1, z2, ... , zn)再和(y1, y2)生成y3，以此类推。因此过去时刻的输出会成为你当前时刻的输入\n",
    "#### encoder\n",
    "sub-layer1: multi-head attention\n",
    "sub-layer2: feed forward\n",
    "每个sub-layer后增加一个residual层和一个layer-normalization层\n",
    "LayerNorm(x + SubLayer(x))\n",
    "每一个层输出的维度d_model = 512（feature的维度）\n",
    "#### why layer-normalization not batch-normalization?\n",
    "batch-normalization对每个batch的每个feature做均值和方差\n",
    "layer-normalization对每个样本做均值和方差\n",
    "对于数据整体有三个维度，batch_size，seq，feature，因为每个seq的长度可能不同\n",
    "如果对每个feature求均值和方差，每个feature对应不同的seq的长度不同\n",
    "#### decoder\n",
    "sub-layer1: masked multi-head attention \n",
    "masked multi-head attention 防止output在i后面的序列影响i的预测\n",
    "sub-layer2: multi-head attention\n",
    "sub-layer3: feed forward\n",
    "每个sub-layer后增加一个residual层和一个layer-normalization层\n",
    "#### positional encoding\n",
    "如何让自注意力机制使用seq的时序\n",
    "input进入模型的第一步是input embedding层，将input的tokens转换为d_model长度的向量\n",
    "但是这些向量并不具有时序信息\n",
    "positional encoding层通过一个cos和一个sin函数为每个position编号了\n",
    "将编号信息与通过了input embedding层的embedding相加，得到的还是d_model长度的向量，但该向量就具备了时序信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### feed forward network\n",
    "#### 前馈神经网络\n",
    "position-wise fully connected feed forward network 其实就是一个简单的MLP \n",
    "具体来说就是1个Linear层 + 一层ReLU + 一个Linear层 因此就是一个单隐藏层的MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE - ViT\n",
    "[paper_link](https://nn.labml.ai/transformers/vit/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer相当于CNN的缺陷\n",
    "CNN有归纳偏置，一种先验的假设 inductive bias\n",
    "locality：假设图片上某个小部分只受到邻近区域的影响\n",
    "translation equivariance：平移不变性，先做卷积再做平移和先做平移再做卷积没有区别\n",
    "因此ViT在中小数据集上的表现不如CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### class token\n",
    "全局表示：Class token 附加到这些图像块的向量序列的开头，经过 Transformer 的多个自注意力层后，它被用来积累全局的图像信息。这是因为在 Transformer 的操作中，每个 token（包括 class token）都有机会与其他所有 token 交互，从而捕获整个图像的上下文信息。\n",
    "\n",
    "分类目的：在经过 Transformer 层处理后，只有 class token 的输出被用于分类任务。更具体地说，class token 的输出被送入一个前馈神经网络（通常是一个简单的线性层），以产生最终的分类预测。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ViT结构\n",
    "#### embedded\n",
    "1. 一张3通道224×224的图片\n",
    "2. 拆成196个16×16×3的patch\n",
    "3. 线性投射层：全连接层，将patch转为token，将2D的图转为一个1D的token序列；每个token维度是768，共196个token\n",
    "4. cls token：借鉴BERT加一个cls token，所以一共1+196=197个token，每个token维度是768\n",
    "5. 为每个token加上位置编码，用的是sum而不是concat，因此不改变token的维度\n",
    "#### LayerNorm\n",
    "#### Multi-Head Self-Attention\n",
    "base版本的ViT有12个head，因此每个k，q，v对应的大小是197×64\n",
    "#### Residual Connection\n",
    "#### LayerNorm\n",
    "#### MLP\n",
    "进行一个维度放大，一般放大四倍，就是197×3072\n",
    "#### Residual Connection\n",
    "#### output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 混合结构\n",
    "通过一个CNN跑出一张feature map，比如ResNet50最后生成一张14×14的特征图，将这张特征图跑一个ViT的结构也可以"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
