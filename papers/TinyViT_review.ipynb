{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315859be81ab3901",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Review of TinyViT: Fast Pretraining Distillation for Small Vision Transformers\n",
    "[ECCV 2022: TinyViT](https://arxiv.org/abs/2207.10666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0b23f08c72f7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 知识蒸馏\n",
    "1.概念\n",
    "是一种模型压缩方法，主要用于将一个大型模型（通常被称为“教师模型”）的知识迁移到一个较小的模型（通常被称为“学生模型”）中。\n",
    "在图像分类的任务中，教师模型可能是一个经过大量数据训练、拥有复杂结构的深度网络，而学生模型则可能是一个轻量级的网络。通过知识蒸馏，学生模型可以学习到教师模型的某些软性标签和特征，从而获得更好的泛化能力。\n",
    "2.缺陷\n",
    "使用蒸馏预训练模型既低效又昂贵，因为大量的计算资源被消耗在每次迭代中通过大型教师模型传递训练数据上，而不是训练目标小型学生。此外，巨大的教师可能占用大部分GPU内存，由于批处理大小有限，显著降低了学生的训练速度。\n",
    "3.本篇论文提出的知识蒸馏方法\n",
    "提前存储稀疏软标签和数据增强信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
