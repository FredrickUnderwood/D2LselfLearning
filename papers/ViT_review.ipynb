{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaca57cc0c0798a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Review of Vision-Transformer\n",
    "[Review of ViT](https://arxiv.org/abs/2305.09880)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538529a7c8bf844d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Abstract\n",
    "ViT又可能替代CNN在计算机视觉领域的许多应用。\n",
    "对比CNN，Transformer可以关注到一张图片上的全局关系（CNN只能关注到卷积核覆盖的小部分之间的关系）。\n",
    "而目前的ViT则更多是CNN和Attention Mechanism（Attention Mechanism是Transformer的核心）的混合，使得ViT可以同时关注到local和global的特征。\n",
    "这篇文章就是介绍目前常见的ViT架构，特别是那些混合ViT架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08eea370123ae3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "首先介绍了计算机视觉领域的现状，并肯定了纯CNN模型为计算机视觉领域带来了革新。（作者罗列了一些CNN在CV三大领域：图片分类、物体检测和语义分割的成功应用的论文）\n",
    "CNN的问题：CNN只关注local-level的空间相关性，这会导致如果要学习的特征更大、更复杂，那CNN的表现就会下降。\n",
    "ViT的发展历程：从2017年[Transformer](https://arxiv.org/abs/1706.03762)的提出。到2018年[自注意力机制被首次应用于CV领域](https://arxiv.org/abs/1802.05751)，但是仅限于local层面。最后到2020年提出[ViT](https://arxiv.org/abs/2010.11929)模型，使得模型可以从全局学习图片的特征，ViT在多个数据集上表现优异，至此CV领域开始更多关注Transformer的应用。\n",
    "ViT对比CNN：CNN有归纳偏置（This  inductive  bias  includes  characteristics  like  translation and  scale  invariance  due  to  the  shared  weights  across  different  spatial  locations.），主要包括locality（桌子旁边一般有椅子）和translation equivariance（一个物体不管怎么移动都能学到相同的特征），这相当于一种先验知识（prior knowledge），是图像的特点，这使得CNN可以在较小的数据集上学到图片的特征，但这也导致了CNN处理全局特特征的不完美。同时，缺乏这些归纳偏置，也导致了ViT对于dataset的规模有较高的要求，且Transformer自身对于算力也有较高的要求。\n",
    "对比ViT和CNN的优势和缺陷，提出[HVT](https://arxiv.org/abs/2206.10589)：Hybrid Vision Transformers，通过卷积层提取local的特征，再通过ViT的自注意力机制提取全局的特征。\n",
    "作者罗列了一些近期的介绍新ViT架构和ViT应用的论文。\n",
    "这篇文章对ViT和HVT模型从架构上做了一个分类（分别是6类ViT和7类HVT），并介绍了他们在CV各个方面的应用，因为我们做的是图片分类领域的工作，因此我们也只关注ViT和HVT在图片分类领域的工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e40b1",
   "metadata": {},
   "source": [
    "## Fundamentals Concepts in ViTs\n",
    "本章2.1-2.4介绍ViT的核心概念。\n",
    "可以参考提出[ViT](https://arxiv.org/abs/2010.11929)模型的文章来讲述ViT的核心概念，这篇文章我前面做过[精读](https://github.com/FredrickUnderwood/D2LselfLearning/blob/main/lecture68_Transformer.ipynb)，参考的是这个[解读视频](https://b23.tv/BGltYvQ)， GitHub上也有很清晰的ViT[代码](https://github.com/lucidrains/vit-pytorch)，这份代码很清晰的表现了注意力机制和图片编码的过程，很适合对于Transformer和注意力机制不熟悉的人进行相关的学习。\n",
    "本章2.5介绍HVT的核心概念。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4282e5d574bf3b8f715e83535c0c211924f7447b47ecaec7e5158fd15c3ee76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
