{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaca57cc0c0798a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Review of Vision-Transformer\n",
    "[Review of ViT](https://arxiv.org/abs/2305.09880)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538529a7c8bf844d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Abstract\n",
    "ViT又可能替代CNN在计算机视觉领域的许多应用。\n",
    "对比CNN，Transformer可以关注到一张图片上的全局关系（CNN只能关注到卷积核覆盖的小部分之间的关系）。\n",
    "而目前的ViT则更多是CNN和Attention Mechanism（Attention Mechanism是Transformer的核心）的混合，使得ViT可以同时关注到local和global的特征。\n",
    "这篇文章就是介绍目前常见的ViT架构，特别是那些混合ViT架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08eea370123ae3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "首先介绍了计算机视觉领域的现状，并肯定了纯CNN模型为计算机视觉领域带来了革新。（作者罗列了一些CNN在CV三大领域：图片分类、物体检测和语义分割的成功应用的论文）\n",
    "CNN的问题：CNN只关注local-level的空间相关性，这会导致如果要学习的特征更大、更复杂，那CNN的表现就会下降。\n",
    "ViT的发展历程：从2017年[Transformer](https://arxiv.org/abs/1706.03762)的提出。到2018年[自注意力机制被首次应用于CV领域](https://arxiv.org/abs/1802.05751)，但是仅限于local层面。最后到2020年提出[ViT](https://arxiv.org/abs/2010.11929)模型，使得模型可以从全局学习图片的特征，ViT在多个数据集上表现优异，至此CV领域开始更多关注Transformer的应用。\n",
    "ViT对比CNN：CNN有归纳偏置（This  inductive  bias  includes  characteristics  like  translation and  scale  invariance  due  to  the  shared  weights  across  different  spatial  locations.），主要包括locality（桌子旁边一般有椅子）和translation equivariance（一个物体不管怎么移动都能学到相同的特征），这相当于一种先验知识（prior knowledge），是图像的特点，这使得CNN可以在较小的数据集上学到图片的特征，但这也导致了CNN处理全局特特征的不完美。同时，缺乏这些归纳偏置，也导致了ViT对于dataset的规模有较高的要求，且Transformer自身对于算力也有较高的要求。\n",
    "对比ViT和CNN的优势和缺陷，提出[HVT](https://arxiv.org/abs/2206.10589)：Hybrid Vision Transformers，通过卷积层提取local的特征，再通过ViT的自注意力机制提取全局的特征。\n",
    "作者罗列了一些近期的介绍新ViT架构和ViT应用的论文。\n",
    "这篇文章对ViT和HVT模型从架构上做了一个分类（分别是6类ViT和7类HVT），并介绍了他们在CV各个方面的应用，因为我们做的是图片分类领域的工作，因此我们也只关注ViT和HVT在图片分类领域的工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e40b1",
   "metadata": {},
   "source": [
    "## Fundamentals Concepts in ViTs\n",
    "本章2.1-2.4介绍ViT的核心概念。\n",
    "可以参考提出[ViT](https://arxiv.org/abs/2010.11929)模型的文章来讲述ViT的核心概念，这篇文章我前面做过[精读](https://github.com/FredrickUnderwood/D2LselfLearning/blob/main/lecture68_Transformer.ipynb)，参考的是这个[解读视频](https://b23.tv/BGltYvQ)， GitHub上也有很清晰的ViT[代码](https://github.com/lucidrains/vit-pytorch)，这份代码很清晰的表现了注意力机制和图片编码的过程，很适合对于Transformer和注意力机制不熟悉的人进行相关的学习。\n",
    "本章2.5介绍HVT的核心概念。\n",
    "1.在Transformer的图像处理中的patching和tokenization阶段，卷积可以捕捉图像的局部特征。\n",
    "2.[CvT](https://arxiv.org/abs/2103.15808)（Convolutional Vision Transformer）中，使用一个基于卷积的proj在image patches中学习空间上的和low-level的信息，然后将卷积输出线性变化为一个序列，进入Transformer结构中。CNN中有一种增加通道数，使图片尺寸变小的卷积层，称为CNN的空间降采样，而CvT使用分层布局，减少token的数量但增加token的宽度也是类似的行为（令牌池），可以减少计算量。\n",
    "3.[CeiT](https://arxiv.org/abs/2103.11816)（Convolution-enhanced Image Transformers）中利用卷积操作扩展low-level的特征，通过的是image-to-token的结构。\n",
    "4.两者的主要区别在于如何结合卷积和Transformer结构。CvT使用卷积来处理低级特征，并通过层次化的Transformer结构进行长范围的信息处理。而CeiT通过混合注意机制在每一层中都结合了卷积和自注意力。\n",
    "5.[CCT](https://arxiv.org/abs/2104.05704)（Compact Convolutional Transformer）中集成了卷积-池化块来进行tokenization，在小数据集上的表现较好。\n",
    "6.[LocalViT](https://arxiv.org/abs/2104.05707)：将depth-wise convolution引入forward中。\n",
    "7.[LeViT](https://arxiv.org/abs/2104.01136)：高速推理。\n",
    "8.[CoAtNet](https://arxiv.org/abs/2106.04803)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architectural level modifications in ViTs\n",
    "table3显示HVT在知名数据集上的准确率，我们做的事图像分类方面的，因此关注ImageNet数据集上的准确率，准确率最高的使CvT和[MaxViT](https://arxiv.org/abs/2204.01697)，都属于作者分类的Hierarchical integration分类。因此主要了解这两个。\n",
    "table2显示的则是ViT的准确率，在ImageNet上准却屡最高的是[CaiT](https://arxiv.org/abs/2103.17239)和[TinyViT](https://arxiv.org/abs/2207.10666)。\n",
    "其中的其中，TinyViT的参数规模和之前使用的EfficientNetB4类似，属于可训练的范畴。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bebf49b931a5595"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applications of ViTs and HVTs\n",
    "1.图像识别\n",
    "2.图像生成\n",
    "3.图像分割\n",
    "4.图像修复\n",
    "5.特征提取\n",
    "6.医疗影像分析\n",
    "7.目标检测\n",
    "8.位置预测"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59c8292826e0b5fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4282e5d574bf3b8f715e83535c0c211924f7447b47ecaec7e5158fd15c3ee76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
